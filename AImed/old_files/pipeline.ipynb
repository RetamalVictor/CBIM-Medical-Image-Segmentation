{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjbvYjifnvfW"
      },
      "outputs": [],
      "source": [
        "!pip install neptune-client>=0.11.0 pytorch-lightning==1.6.4 scikit-plot==0.3.7 torchvision>=0.6\n",
        "!pip install kornia\n",
        "!git clone https://github.com/RetamalVictor/CBIM-Medical-Image-Segmentation.git && pip install SimpleITK==2.0.0 --quiet\n",
        "!cd /content/CBIM-Medical-Image-Segmentation && pip install -r requirement.txt --quiet\n",
        "%cd /content/CBIM-Medical-Image-Segmentation/\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import yaml\n",
        "import argparse\n",
        "import SimpleITK as sitk\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import tensor\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from model.dim2.unet import *\n",
        "from model.dim2.utnetv2 import *\n",
        "from training.dataset.dim2.dataset_camus import CAMUSDataset\n",
        "import kornia as K\n",
        "import pytorch_lightning as pl\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from google.colab import drive\n",
        "from torchvision.utils import make_grid\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import SimpleITK as sitk\n",
        "import yaml\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from neptune.new.types import File\n",
        "from training import augmentation\n",
        "from training import losses\n",
        "from pytorch_lightning.loggers import NeptuneLogger\n",
        "import cv2         \n",
        "import skimage.exposure\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# these remain constant\n",
        "filepath = \"/content/drive/MyDrive/AI4MED/training/training/\"\n",
        "filepath_checkpoint = \"/content/drive/MyDrive/AI4MED/model-checkpoints/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt9QwzZtpfnt"
      },
      "source": [
        "##**Prepare data and create dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4baU7iDSoXHu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "def gaussian_noise(tensor_img, std, mean=0):\n",
        "\n",
        "    random = torch.randn(tensor_img.shape).to(tensor_img.device) * std + mean\n",
        "\n",
        "    return tensor_img + random\n",
        "\n",
        "def brightness_additive(tensor_img, std, mean=0, per_channel=False):\n",
        "\n",
        "    if per_channel:\n",
        "        C = tensor_img.shape[1]\n",
        "    else:\n",
        "        C = 1\n",
        "\n",
        "    if len(tensor_img.shape) == 5:\n",
        "        rand_brightness = torch.normal(mean, std, size=(1, C, 1, 1, 1)).to(\n",
        "            tensor_img.device\n",
        "        )\n",
        "    elif len(tensor_img.shape) == 4:\n",
        "        rand_brightness = torch.normal(mean, std, size=(1, C, 1, 1)).to(\n",
        "            tensor_img.device\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid input tensor dimension, should be 5d for volume image or 4d for 2d image\"\n",
        "        )\n",
        "    return tensor_img + rand_brightness\n",
        "\n",
        "class CAMUSDataset(Dataset):\n",
        "    def __init__(self, args, mode=None, seed=0, only_quality=True):\n",
        "\n",
        "        self.mode = mode\n",
        "        self.doAugmentation = args[\"augs\"]\n",
        "        self.only_quality = only_quality\n",
        "        self.args = args\n",
        "        self.img_slice_list= []\n",
        "        self.lab_slice_list= []\n",
        "\n",
        "        self.sizes = []\n",
        "\n",
        "        self.filepath= \"/content/drive/MyDrive/AI4MED/training/training/\"\n",
        "        self.filepath_test = \"/content/drive/MyDrive/AI4MED/Copy_of_testing/testing/\" \n",
        "\n",
        "        with open(os.path.join(args[\"data_root\"], \"list\", \"dataset.yaml\"), \"r\") as f:\n",
        "            img_name_list = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "\n",
        "        camus_test_names = img_name_list[0:50]    \n",
        "\n",
        "        random.Random(seed).shuffle(img_name_list)\n",
        "\n",
        "        quality_patients_2CH = []\n",
        "        quality_patients_4CH = []\n",
        "        print(\"Selecting good quality samples\")\n",
        "\n",
        "        img_names = []\n",
        "        if only_quality:\n",
        "            for patient in tqdm(img_name_list):\n",
        "                with open(\n",
        "                    os.path.join(args[\"data_info\"], patient, \"Info_2CH.cfg\")\n",
        "                ) as info2:\n",
        "                    i2 = yaml.safe_load(info2)\n",
        "                    if i2[\"ImageQuality\"] == \"Good\" or i2[\"ImageQuality\"] == \"Medium\":\n",
        "                        quality_patients_2CH.append(patient)\n",
        "                        \n",
        "                        img_names.append(patient)\n",
        "                with open(\n",
        "                    os.path.join(args[\"data_info\"], patient, \"Info_4CH.cfg\")\n",
        "                ) as info4:\n",
        "                    i4 = yaml.safe_load(info4)\n",
        "                    if i4[\"ImageQuality\"] == \"Good\" or i4[\"ImageQuality\"] == \"Medium\":\n",
        "                        quality_patients_4CH.append(patient)\n",
        "                        img_names.append(patient)\n",
        "\n",
        "        length = len(img_name_list)\n",
        "        img_names_list = img_names\n",
        "        print(f\"The length of the patient list is {length}\")\n",
        "        if only_quality:\n",
        "            print(\n",
        "                f\"It will include {len(quality_patients_2CH)*2 + len(quality_patients_4CH)*2} good quality samples\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"It will include {length * 4} samples\")\n",
        "\n",
        "        test_name_list = img_name_list[: args[\"test_size\"]]\n",
        "        train_name_list = list(set(img_name_list) - set(test_name_list))\n",
        "\n",
        "        print(\"start loading data\")\n",
        "\n",
        "        path = filepath\n",
        "        img_list = []\n",
        "        lab_list = []\n",
        "        idx = [\"_2CH_ED.mhd\", \"_2CH_ES.mhd\", \"_4CH_ED.mhd\", \"_4CH_ES.mhd\"]\n",
        "        if mode == 'train':\n",
        "          # Load training\n",
        "          for name in tqdm(train_name_list):\n",
        "              selected_images = []\n",
        "              if only_quality:\n",
        "                  if name in quality_patients_2CH:\n",
        "                      selected_images += idx[:2]\n",
        "                  if name in quality_patients_4CH:\n",
        "                      selected_images += idx[2:]\n",
        "              else:\n",
        "                  selected_images = idx\n",
        "\n",
        "              for id in selected_images:\n",
        "                  img_name = name + id\n",
        "                  lab_name = name + id.replace(\".\", \"_gt.\")\n",
        "\n",
        "                  itk_img =  cv2.resize(sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(path+\"/\"+name+\"/\", img_name), sitk.sitkFloat32))[0,:,:], dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "                  itk_lab = cv2.resize(sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(path+\"/\"+name+\"/\", lab_name), sitk.sitkFloat32))[0,:,:], dsize=(256, 256), interpolation=cv2.INTER_NEAREST)\n",
        "                  itk_lab= np.reshape(itk_lab,(1,256,256))\n",
        "                  itk_img = np.reshape(itk_img,(1,256,256))/255\n",
        "                  img, lab = self.preprocess(itk_img, itk_lab)\n",
        "                  img_list.append(img)\n",
        "                  lab_list.append(lab)\n",
        "\n",
        "\n",
        "          for i in range(len(img_list)):\n",
        "                self.img_slice_list.append(img_list[i][0])\n",
        "                self.lab_slice_list.append(lab_list[i][0])\n",
        "          \n",
        "          if self.doAugmentation:\n",
        "            print(\"Augmenting now\")\n",
        "            self.create_augmentations()\n",
        "\n",
        "          print(\"Train done, length of dataset:\", len(self.img_slice_list))\n",
        "\n",
        "        ###### actual test set from camus with no labels\n",
        "        elif mode =='camus':\n",
        "          self.all_names_save = []\n",
        "          for name in tqdm(camus_test_names):\n",
        "              selected_images = idx\n",
        "              for id in selected_images:\n",
        "                  img_name = name + id\n",
        "                  self.all_names_save.append(img_name.replace(\".mhd\", \"\"))\n",
        "                  self.sizes.append(sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(self.filepath_test+\"/\"+name+\"/\", img_name), sitk.sitkFloat32))[0,:,:].shape)\n",
        "                  itk_img =  cv2.resize(sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(self.filepath_test+\"/\"+name+\"/\", img_name), sitk.sitkFloat32))[0,:,:], dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "                  itk_img = np.reshape(itk_img,(1,256,256))/255\n",
        "                  img, lab = self.preprocess(itk_img, itk_img)\n",
        "                  img_list.append(img)\n",
        "                  lab_list.append(lab)\n",
        "          for i in range(len(img_list)):\n",
        "              self.img_slice_list.append(img_list[i][0])\n",
        "              self.lab_slice_list.append(lab_list[i][0])\n",
        "\n",
        "        elif mode == 'test':\n",
        "\n",
        "          self.all_names_save = []\n",
        "          # Load tests\n",
        "          for name in tqdm(test_name_list):\n",
        "              selected_images = []\n",
        "              if only_quality:\n",
        "                  if name in quality_patients_2CH:\n",
        "                      selected_images += idx[:2]\n",
        "                  if name in quality_patients_4CH:\n",
        "                      selected_images += idx[2:]\n",
        "              else:\n",
        "                  selected_images = idx\n",
        "\n",
        "              for id in selected_images:\n",
        "\n",
        "                  img_name = name + id\n",
        "                  self.all_names_save.append(img_name)\n",
        "                  lab_name = name + id.replace(\".\", \"_gt.\")\n",
        "                  self.sizes.append(sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(path+\"/\"+name+\"/\", lab_name),sitk.sitkFloat32))[0,:,:].shape)\n",
        "                  itk_img =  cv2.resize(sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(path+\"/\"+name+\"/\", img_name), sitk.sitkFloat32))[0,:,:], dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "                  itk_lab = cv2.resize(sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(path+\"/\"+name+\"/\", lab_name), sitk.sitkFloat32))[0,:,:], dsize=(256, 256), interpolation=cv2.INTER_NEAREST)\n",
        "                  itk_lab= np.reshape(itk_lab,(1,256,256))\n",
        "                  itk_img = np.reshape(itk_img,(1,256,256))/255\n",
        "\n",
        "                  img, lab = self.preprocess(itk_img, itk_lab)\n",
        "\n",
        "                  img_list.append(img)\n",
        "                  lab_list.append(lab)\n",
        "\n",
        "          for i in range(len(img_list)):\n",
        "              self.img_slice_list.append(img_list[i][0])\n",
        "              self.lab_slice_list.append(lab_list[i][0])\n",
        "\n",
        "          print(\"Test done, length of dataset:\", len(self.img_slice_list))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_slice_list)\n",
        "\n",
        "    def preprocess(self, img, lab):\n",
        "\n",
        "        import torchvision.transforms as T\n",
        "        tensor_img = torch.from_numpy(img).float()\n",
        "        tensor_lab = torch.from_numpy(lab).long()\n",
        "\n",
        "        return tensor_img, tensor_lab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "          tensor_img = self.img_slice_list[idx]\n",
        "          tensor_lab = self.lab_slice_list[idx]\n",
        "          tensor_img = tensor_img.unsqueeze(0)\n",
        "          tensor_lab = tensor_lab.unsqueeze(0)\n",
        "          assert tensor_img.shape == tensor_lab.shape\n",
        "\n",
        "          return torch.clamp(tensor_img, min=0.0, max=1.0), tensor_lab\n",
        "\n",
        "    def create_augmentations(self):\n",
        "        \n",
        "        import torchvision.transforms as T\n",
        "\n",
        "        total = len(self.img_slice_list)\n",
        "        new_imgs = []\n",
        "        new_labs = []\n",
        "\n",
        "        s1 = self.args[\"s1\"]\n",
        "        s2 = self.args[\"s2\"]\n",
        "        t = self.args[\"t\"]\n",
        "        rot = self.args[\"rotation\"]\n",
        "        for i in range(total):\n",
        "        \n",
        "                    ######## affine transformation \n",
        "                    # use the same transformation for ground truth label and image\n",
        "                    affine_transfomer = T.RandomAffine(degrees=(-rot, rot), translate=(t, t), scale=(s1, s2))\n",
        "                    new = torch.cat((self.img_slice_list[i].unsqueeze(0), self.lab_slice_list[i].unsqueeze(0)), dim=0)\n",
        "                    transformed = affine_transfomer(new.unsqueeze(dim=1))\n",
        "\n",
        "                    img_aug = torch.clamp(transformed[0].squeeze(), min=0.0, max=1.0)\n",
        "                    new_imgs.append(img_aug)\n",
        "                    new_labs.append(transformed[1].squeeze().int())              \n",
        "                    \n",
        "                    ###### SNR augmentation\n",
        "                    if self.args['SNR'] == True: \n",
        "\n",
        "                      # append label to set, this is unchanged\n",
        "                      new_labs.append(self.lab_slice_list[i].int())\n",
        "\n",
        "                      label = self.lab_slice_list[i]\n",
        "                      img =  self.img_slice_list[i]\n",
        "\n",
        "                      ##### mask 3 pixels around the triangular border otherwise it will\n",
        "                      ##### be extremely visible in in the monogenic signal\n",
        "                      mask = np.where(label > 0, 1, 0)\n",
        "                      for i in range(256):\n",
        "                        for j in range(256):\n",
        "                            if j == 0 and mask[i][j] == 1:\n",
        "                              mask[i][j+1] = 0\n",
        "                            if j < 254:\n",
        "                                  if mask[i][j] == 0 :\n",
        "                                    mask[i][j+1] = 0           \n",
        "                                    break\n",
        "                      for i in range(256):\n",
        "                        for j in range(256):\n",
        "                            if j < 254:\n",
        "                                  if mask[i][j] == 0 and mask[i][j+1]==1:\n",
        "                                    mask[i][j+1] = 0           \n",
        "                                    break\n",
        "                      for i in range(256):\n",
        "                        for j in range(256):\n",
        "                            if j < 254:\n",
        "                                  if mask[i][j] == 0 and mask[i][j+1]==1:\n",
        "                                    mask[i][j+1] = 0           \n",
        "                                    break\n",
        "                      for i in range(255, -1, -1):\n",
        "                        for j in range(255, -1, -1):\n",
        "                            if j == 255 and mask[i][j] == 1:\n",
        "                              mask[i][j] = 0  \n",
        "                      for i in range(255, -1, -1):\n",
        "                        for j in range(255, -1, -1):\n",
        "                            if j > 0:\n",
        "                                  if mask[i][j] == 0 and mask[i][j-1] == 1:\n",
        "                                    mask[i][j-1] = 0           \n",
        "                                    break \n",
        "                      for i in range(255, 252, -1):\n",
        "                        for j in range(255, -1, -1):\n",
        "                          mask[i][j] = 0\n",
        "\n",
        "                      rows, cols = img.shape\n",
        "\n",
        "                      # the higher the wavelength and sigma the more \"details\" are removed\n",
        "                      logGabor, logGabor_H1, logGabor_H2 = mf.monogenic_scale(cols=cols,rows=rows,ss=1,minWaveLength=3,mult=1.8,sigmaOnf=0.2)\n",
        "\n",
        "                      IM = fft2(img)  \n",
        "                      IMF = IM * logGabor  \n",
        "\n",
        "                      IMH1=IM*logGabor_H1\n",
        "                      IMH2=IM*logGabor_H2\n",
        "\n",
        "                      f = np.real(ifft2(IMF))  \n",
        "                      h1= np.real(ifft2(IMH1))\n",
        "                      h2= np.real(ifft2(IMH2))\n",
        "\n",
        "                      ##### LEM\n",
        "                      LEM = torch.FloatTensor(f * f + h1 * h1 + h2 * h2)\n",
        "                      \n",
        "                      mask = torch.tensor(mask)\n",
        "                      signal = LEM * mask.float()\n",
        "\n",
        "                      #### create gaussian smoothed edges for every ground truth label\n",
        "                      label0 = torch.where(label == 0, -1, 0) \n",
        "                      label0 = torch.where(label0 ==-1, 1, 0) # background\n",
        "                      label1 = torch.where(label == 1, 1, 0) # big chamber\n",
        "                      label2 = torch.where(label == 2, 1, 0) # white thing around\n",
        "                      label3 = torch.where(label == 3, 1, 0) # little one at the bottom\n",
        "\n",
        "                      # blur label 1\n",
        "                      blur = cv2.GaussianBlur(label1.float().numpy(), (0,0), sigmaX=5, sigmaY=5, borderType = cv2.BORDER_DEFAULT)\n",
        "                      label1 = skimage.exposure.rescale_intensity(blur, in_range=(0.5,1), out_range=(0,1))\n",
        "\n",
        "                      # blur label 2\n",
        "                      blur = cv2.GaussianBlur(label2.float().numpy(), (0,0), sigmaX=5, sigmaY=5, borderType = cv2.BORDER_DEFAULT)\n",
        "                      label2 = skimage.exposure.rescale_intensity(blur, in_range=(0.5,1), out_range=(0,1))\n",
        "\n",
        "                      # blur label 3\n",
        "                      blur = cv2.GaussianBlur(label3.float().numpy(), (0,0), sigmaX=5, sigmaY=5, borderType = cv2.BORDER_DEFAULT)\n",
        "                      label3 = skimage.exposure.rescale_intensity(blur, in_range=(0.5,1), out_range=(0,1))\n",
        "\n",
        "                      # blur label 4\n",
        "                      blur = cv2.GaussianBlur(label0.float().numpy(), (0,0), sigmaX=5, sigmaY=5, borderType = cv2.BORDER_DEFAULT)\n",
        "                      label0 = skimage.exposure.rescale_intensity(blur, in_range=(0.5,1), out_range=(0,1))\n",
        "\n",
        "                      # create random scaling factors for every label segmentation seperately\n",
        "                      random_scale = torch.randint(-1, 3, (4,))\n",
        "\n",
        "                      # build up the new signal by multiplying the scaling factors with the EFM\n",
        "                      signal2 = ((signal *random_scale[0]* label0) + (signal * random_scale[1] *label1) + (signal*random_scale[2]*label2) + (signal*random_scale[3]*label3))\n",
        "                      aug = torch.tensor(img) + signal2\n",
        "                      img_aug = torch.clamp(aug, min=0.0, max=1.0)\n",
        "                      new_imgs.append(img_aug)\n",
        "                    \n",
        "        for jj in range(len(new_imgs)):\n",
        "          self.img_slice_list.append(new_imgs[jj])\n",
        "          self.lab_slice_list.append(new_labs[jj].long())    \n",
        "\n",
        "\n",
        "# datamodule for camus dataset\n",
        "class CAMUS_DATA(pl.LightningDataModule):          \n",
        "    def __init__(self, data_root= \"/content/drive/MyDrive/AI4MED/tgt_dir\",t=0.3, s1 = 0.6, s2 = 1.5, rotation=20, batch_size=8, training_size= [256,256], test_size=10, SNR=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.batch_size = batch_size\n",
        "        self.training_size = training_size   \n",
        "\n",
        "        self.args = {\n",
        "          \"data_root\": data_root,\n",
        "          \"data_info\": \"/content/drive/MyDrive/AI4MED/training/training\",\n",
        "          \"training_size\": training_size,\n",
        "          'test_size': test_size,\n",
        "          'rotation':rotation, \n",
        "          's1':s1,\n",
        "          's2':s1,\n",
        "          't':t,\n",
        "          \"augs\": True,\n",
        "          \"SNR\": SNR\n",
        "        }\n",
        "      \n",
        "    def train_dataloader(self):\n",
        "        data = CAMUSDataset(self.args, mode='train')\n",
        "        return DataLoader(data, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        data = CAMUSDataset(self.args, mode='test')\n",
        "        return DataLoader(data, batch_size=self.batch_size, shuffle=False)   \n",
        "\n",
        "    def test_dataloader(self):\n",
        "        data = CAMUSDataset(self.args, mode='test')\n",
        "        return DataLoader(data, batch_size=self.batch_size, shuffle=False)   \n",
        "\n",
        "    #### for using the official test set of camus challenge\n",
        "    #def test_dataloader(self):\n",
        "    #    data = CAMUSDataset(self.args, mode='camus')\n",
        "    #    return DataLoader(data, batch_size=self.batch_size, shuffle=False)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiUhy-0WqnMq"
      },
      "source": [
        "##**Segmentation model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHWaX8T8qv1_"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules import PairwiseDistance\n",
        "class Segmentation(pl.LightningModule):\n",
        "    def __init__(self, model=\"UTNetV2\", loss_function=\"CE+EDGE\", weight=[0.5, 1, 1, 1], lr=0.0005, batch_size=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.epoch = 0\n",
        "\n",
        "        self.name_list_all = [\"_2CH_ED\", \"_2CH_ES\", \"_4CH_ED\", \"_4CH_ES\"]*10\n",
        "        self.ED = []\n",
        "        self.ES = []\n",
        "        self.name = model\n",
        "        self.weight = weight\n",
        "        self.flag = None\n",
        "        self.classes = 3 # [0, 1, 2, 3] for normalizing between 0 and 1\n",
        "        self.batch_size = batch_size\n",
        "        self.val_batch_size = 1\n",
        "        self.loss_function = loss_function\n",
        "        self.weight = weight\n",
        "        self.lr = lr\n",
        "        \n",
        "        if model == \"UTNetV2\":\n",
        "          self.model = UTNetV2(1,4)\n",
        "          self.filepath_logs = \"/content/drive/MyDrive/AI4MED/logs/UTnetv2/\"\n",
        "\n",
        "        if model == \"Unet\":\n",
        "          self.model = UNet(1,4)   \n",
        "          self.filepath_logs = \"/content/drive/MyDrive/AI4MED/logs/Unet/\"   \n",
        "\n",
        "        if loss_function == \"CE\":\n",
        "          self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(self.weight))\n",
        "          self.flag = 0\n",
        "\n",
        "        if loss_function == \"CE+EDGE\":\n",
        "          self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(self.weight))\n",
        "          self.criterion2 = nn.CrossEntropyLoss(ignore_index = -10)\n",
        "          self.criterion3 = losses.DiceLoss()\n",
        "          self.flag = 1\n",
        "\n",
        "        elif loss_function == \"Dice\":\n",
        "          self.criterion = losses.DiceLoss()\n",
        "          self.flag = 2\n",
        "\n",
        "        elif loss_function == \"Focal\":\n",
        "          self.criterion = losses.FocalLoss(class_num=4, gamma=2)\n",
        "          self.flag = 3\n",
        "\n",
        "        elif loss_function == \"DICE+CE\":\n",
        "          self.criterion2 = losses.DiceLoss()\n",
        "          self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(self.weight))\n",
        "          self.flag = 4\n",
        "\n",
        "    def multi_class_dice_score(self, img, labels, class_labels=[1,2,3]):\n",
        "        \"\"\" Given an image and a label compute the dice score over\n",
        "        multiple class volumes. You can specify which classes dice\n",
        "        should be computed for. Don't use zero because it's the background.\"\"\"\n",
        "\n",
        "        total_volume = 0.\n",
        "        total_intersect_volume = 0.\n",
        "        \n",
        "        outputs = []\n",
        "        for label in class_labels:\n",
        "            img_bool = img.flatten() == label\n",
        "            labels_bool = labels.flatten() == label\n",
        "\n",
        "            volume = sum(img_bool) + sum(labels_bool)\n",
        "            intersect_volume = sum(img_bool & labels_bool)\n",
        "\n",
        "            total_volume += volume\n",
        "            total_intersect_volume += intersect_volume\n",
        "\n",
        "            outputs.append(2 * intersect_volume / volume)\n",
        "\n",
        "        return 2 * total_intersect_volume / total_volume, outputs\n",
        "\n",
        "\n",
        "    def multi_class_jaccard(self, img, labels, class_labels=[1,2,3]):\n",
        "      \"\"\" Jaccard metric defined for two sets as |A and B| / |A or B|\"\"\"\n",
        "\n",
        "      total_union_volume = 0.\n",
        "      total_intersect_volume = 0.\n",
        "      \n",
        "      outputs = []\n",
        "      for label in class_labels:\n",
        "          img_bool = img.flatten() == label\n",
        "          labels_bool = labels.flatten() == label\n",
        "\n",
        "          union_volume = sum(img_bool | labels_bool)\n",
        "          intersect_volume = sum(img_bool & labels_bool)\n",
        "\n",
        "          total_union_volume += union_volume\n",
        "          total_intersect_volume += intersect_volume\n",
        "\n",
        "          outputs.append(intersect_volume/union_volume)\n",
        "\n",
        "      return total_intersect_volume / total_union_volume, outputs\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        img, label= batch\n",
        "\n",
        "        logits = self.model(img.float())              \n",
        "        # regular weighted CE Loss\n",
        "        if self.flag == 0:\n",
        "          train_loss = self.criterion(logits, label.squeeze(1))\n",
        "          self.log(\"CE_loss\", train_loss)\n",
        "          self.log(\"train_loss\", train_loss)\n",
        "\n",
        "        # custom loss incorporating edges\n",
        "        if self.flag == 1:\n",
        "          CE_loss = self.criterion(logits, label.squeeze(1))\n",
        "          # find the edge pixels for the segmentation classes\n",
        "          x_sobel = K.filters.sobel(label/self.classes)\n",
        "          reverse = 1.0 - x_sobel\n",
        "          # set non-edge pixels to -10 such that these will be ignored in the loss\n",
        "          edges = torch.where(reverse > 0.9989, -10, label).squeeze(1)\n",
        "\n",
        "          reg_loss = self.criterion2(logits, edges)  \n",
        "          D_loss = self.criterion3(logits, label)\n",
        "          train_loss = CE_loss +  0.2 * reg_loss + D_loss\n",
        "\n",
        "          self.log(\"train_loss\", train_loss)\n",
        "          self.log(\"CE_loss\", CE_loss)\n",
        "          self.log(\"reg_loss\", reg_loss)\n",
        "          self.log(\"train_d_loss\", D_loss)\n",
        "\n",
        "        # Dice loss\n",
        "        if self.flag == 2:\n",
        "          train_loss = self.criterion(logits, label)\n",
        "          self.log(\"dice_loss\", train_loss)\n",
        "          self.log(\"train_loss\", train_loss)\n",
        "\n",
        "        # focal loss\n",
        "        if self.flag == 3:\n",
        "          train_loss = self.criterion(logits, label.squeeze(1))\n",
        "          self.log(\"focal_loss\", train_loss)\n",
        "          self.log(\"train_loss\", train_loss)         \n",
        "\n",
        "        # dice loss + CE loss\n",
        "        if self.flag == 4:\n",
        "          CE_loss = self.criterion(logits, label.squeeze(1))\n",
        "          D_loss = self.criterion2(logits, label)\n",
        "          train_loss =  CE_loss + D_loss\n",
        "          self.log(\"train_loss\", train_loss)\n",
        "          self.log(\"CE_loss\", CE_loss)\n",
        "          self.log(\"DL_loss\", D_loss)\n",
        "     \n",
        "        new = torch.cat((img, label/self.classes), dim=0)\n",
        "        vis = torchvision.utils.make_grid(new, nrow=self.batch_size, padding=5)\n",
        "        out_np = K.utils.tensor_to_image(vis)\n",
        "        #self.logger.experiment[\"train/segmentations_pairs\"].log(File.as_image(out_np))   \n",
        "        return train_loss\n",
        "\n",
        "    # runs metrics on validation set \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        img = batch[0]\n",
        "        label = batch[1]\n",
        "        logits = self.model(img.float())\n",
        "\n",
        "        if self.flag == 0 or self.flag == 3:\n",
        "          val_loss = self.criterion(logits, label.squeeze(1))\n",
        "\n",
        "        if self.flag == 1:\n",
        "          CE_loss = self.criterion(logits, label.squeeze(1))\n",
        "          # find the edge pixels for the segmentation classes\n",
        "          x_sobel = K.filters.sobel(label/self.classes)\n",
        "          reverse = 1.0 - x_sobel\n",
        "\n",
        "          # set non-edge pixels to -10 such that these will be ignored in the loss\n",
        "          edges = torch.where(reverse > 0.9989, -10, label).squeeze(1)\n",
        "          reg_loss = self.criterion2(logits, edges)  \n",
        "\n",
        "          # visualize the edges\n",
        "          binary = torch.where(reverse > 0.9989, 0, 1.0)\n",
        "          out_edge = torchvision.utils.make_grid(edges, nrow=self.batch_size, padding=5)\n",
        "          out_np = K.utils.tensor_to_image(out_edge)\n",
        "\n",
        "          val_loss = CE_loss + 0.2* reg_loss\n",
        "          self.log(\"test_CE_loss\", CE_loss)\n",
        "          self.log(\"test_reg_loss\", reg_loss)\n",
        "\n",
        "        if self.flag == 2:\n",
        "          val_loss = self.criterion(logits, label)\n",
        "\n",
        "        if self.flag == 4:\n",
        "          CE_loss = self.criterion(logits, label.squeeze(1))\n",
        "          D_loss = self.criterion2(logits, label)\n",
        "          val_loss =  CE_loss + D_loss\n",
        "          self.log(\"val_CE_loss\", CE_loss)\n",
        "          self.log(\"test_DL_loss\", D_loss)    \n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        self.log(\"test_loss\", val_loss)\n",
        "\n",
        "        # need sizes for resize back to original size compute actual distances \n",
        "        sizes = [(843, 512), (843, 512), (1038, 630), (1038, 630), (973, 591), (973, 591), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1232, 748), (1232, 748), (1232, 748), (1232, 748), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (908, 641), (908, 641), (908, 641), (908, 641), (973, 590), (973, 590), (973, 590), (973, 590), (778, 472), (778, 472), (778, 472), (778, 472)]\n",
        "\n",
        "        preds= torch.tensor(cv2.resize(preds.squeeze().cpu().numpy(), dsize=(sizes[self.epoch][1],sizes[self.epoch][0]), interpolation=cv2.INTER_NEAREST)).float().unsqueeze(0)\n",
        "        label = torch.tensor(cv2.resize(label.squeeze().cpu().numpy(), dsize=(sizes[self.epoch][1],sizes[self.epoch][0]), interpolation=cv2.INTER_NEAREST)).int().unsqueeze(0)\n",
        "\n",
        "        u = torch.argwhere(label.squeeze()==1).cpu().numpy()\n",
        "        v = torch.argwhere(preds.squeeze()==1).cpu().numpy()\n",
        "\n",
        "        from scipy.spatial.distance import directed_hausdorff\n",
        "        HF1 = max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])\n",
        "\n",
        "        ##### camus dataset-> 0.145x0.154mm^2\n",
        "        HF1 = HF1*0.154\n",
        "\n",
        "        u = torch.argwhere(label.squeeze()==2).cpu().numpy()\n",
        "        v = torch.argwhere(preds.squeeze()==2).cpu().numpy()\n",
        "\n",
        "        HF2 = max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])\n",
        "        HF2 = HF2*0.154\n",
        "\n",
        "        u = torch.argwhere(label.squeeze()==3).cpu().numpy()\n",
        "        v = torch.argwhere(preds.squeeze()==3).cpu().numpy()\n",
        "\n",
        "        HF3 = max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])\n",
        "        HF3 = HF3*0.154\n",
        "\n",
        "        D, total1 =  self.multi_class_dice_score(preds, label.squeeze(1))\n",
        "\n",
        "        if self.epoch % 4 == 0 or self.epoch % 4 ==2:\n",
        "          self.ED.append([total1, HF1, HF2, HF3])\n",
        "        else:\n",
        "          self.ES.append([total1, HF1, HF2, HF3])\n",
        "\n",
        "        self.epoch = self.epoch + 1\n",
        "        return val_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):  \n",
        "        img = batch[0]\n",
        "        label = batch[1]\n",
        "        logits = self.model(img.float())\n",
        "\n",
        "        if self.flag == 0 or self.flag == 3:\n",
        "          val_loss = self.criterion(logits.float(), label.squeeze(1))\n",
        "          self.log(\"D_loss_val\", val_loss)\n",
        "\n",
        "        if self.flag == 1:\n",
        "          val_CE_loss = self.criterion(logits, label.squeeze(1))\n",
        "          # find the edge pixels for the segmentation classes\n",
        "          x_sobel = K.filters.sobel(label/self.classes)\n",
        "          reverse = 1.0 - x_sobel\n",
        "\n",
        "          # set non-edge pixels to -10 such that these will be ignored in the loss\n",
        "          edges = torch.where(reverse > 0.9989, -10, label).squeeze(1)\n",
        "          reg_loss = self.criterion2(logits, edges)\n",
        "\n",
        "          # visualize the edges\n",
        "          binary = torch.where(reverse > 0.9989, 0, 1.0)\n",
        "          preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "          new = torch.cat((preds.unsqueeze(1)/self.classes, reverse/self.classes), dim=0)\n",
        "          vis = torchvision.utils.make_grid(new, nrow=self.batch_size, padding=5)\n",
        "          out_np = K.utils.tensor_to_image(vis)   \n",
        "        \n",
        "          D_loss_val= self.criterion3(logits, label)\n",
        "          val_loss = val_CE_loss +  0.2 * reg_loss + D_loss_val\n",
        "\n",
        "          self.log(\"val_CE_loss\", val_CE_loss)\n",
        "          self.log(\"val_reg_loss\", reg_loss)\n",
        "          self.log(\"D_loss_val\", D_loss_val)\n",
        "\n",
        "        if self.flag == 2:\n",
        "          val_loss = self.criterion(logits, label)\n",
        "          self.log(\"D_loss_val\", val_loss)\n",
        "\n",
        "        if self.flag == 4:\n",
        "          CE_loss = self.criterion(logits, label.squeeze(1))\n",
        "          D_loss = self.criterion2(logits, label)\n",
        "          val_loss =  CE_loss + D_loss\n",
        "          self.log(\"val_CE_loss\", CE_loss)\n",
        "          self.log(\"D_loss_val\", D_loss)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        self.log(\"val_loss\", val_loss)\n",
        "\n",
        "        #D, total =  self.multi_class_dice_score(preds, label.squeeze(1))\n",
        "        self.log(\"D\",D)\n",
        "\n",
        "        # log predictions and label \n",
        "        #new = torch.cat((preds.unsqueeze(1)/self.classes, label/self.classes), dim=0)\n",
        "        #vis = torchvision.utils.make_grid(new, nrow=self.batch_size, padding=5)\n",
        "        #out_np = K.utils.tensor_to_image(vis)      \n",
        "        #self.logger.experiment[\"validation/segmentations_pairs\"].log(File.as_image(out_np)) \n",
        "\n",
        "        # log absolute difference \n",
        "        #vis = torchvision.utils.make_grid((abs(label.squeeze(1)[0].detach()-preds[0].detach()))/self.classes, nrow=self.batch_size, padding=5)\n",
        "        #out_np = K.utils.tensor_to_image(vis)\n",
        "        #self.logger.experiment[\"validation/absolute_differences\"].log(File.as_image(out_np)) \n",
        "       \n",
        "        return val_loss\n",
        "\n",
        "    def make_camus_pred_test(self, batch, batch_idx):  \n",
        "        img = batch[0]       \n",
        "        logits = self.model(img.float())  \n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        sizes =[(1102, 669), (1102, 669), (1102, 669), (1102, 669), (778, 499), (778, 499), (843, 541), (843, 541), (973, 591), (973, 591), (1038, 630), (1038, 630), (1168, 708), (1168, 708), (1168, 708), (1168, 708), (908, 551), (908, 551), (908, 551), (908, 551), (843, 512), (843, 512), (843, 512), (843, 512), (843, 512), (843, 512), (843, 512), (843, 512), (908, 551), (908, 551), (908, 551), (908, 551), (908, 551), (908, 551), (908, 551), (908, 551), (973, 591), (973, 591), (973, 591), (973, 591), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (908, 551), (908, 551), (908, 551), (908, 551), (843, 451), (843, 451), (843, 451), (843, 451), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (714, 433), (714, 433), (714, 433), (714, 433), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1168, 708), (1168, 708), (1168, 708), (1168, 708), (1038, 630), (1038, 630), (973, 590), (973, 590), (843, 451), (843, 451), (843, 451), (843, 451), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (908, 582), (908, 582), (908, 551), (908, 551), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (973, 590), (973, 590), (973, 590), (973, 590), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1232, 747), (1232, 747), (1232, 747), (1232, 747), (1232, 747), (1232, 747), (1232, 747), (1232, 747), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1297, 787), (1297, 787), (1297, 787), (1297, 787), (908, 551), (908, 551), (908, 551), (908, 551), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (973, 590), (973, 590), (973, 590), (973, 590), (1038, 630), (1038, 630), (908, 551), (908, 551), (908, 551), (908, 551), (1038, 630), (1038, 630), (973, 590), (973, 590), (779, 472), (779, 472), (779, 472), (779, 472), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (908, 487), (908, 487), (908, 487), (908, 487), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (973, 590), (973, 590), (843, 512), (843, 512), (843, 512), (843, 512), (843, 512), (843, 512), (843, 595), (843, 595), (843, 595), (843, 595), (1427, 865), (1427, 865), (1427, 865), (1427, 865), (584, 354), (584, 354), (584, 354), (584, 354), (843, 512), (843, 512), (843, 512), (843, 512), (843, 595), (843, 595), (843, 595), (843, 595)]     \n",
        "        \n",
        "        preds= torch.tensor(cv2.resize(preds.squeeze().cpu().numpy(), dsize=(sizes[self.epoch][1],sizes[self.epoch][0]), interpolation=cv2.INTER_NEAREST)).float().unsqueeze(0)\n",
        "        plt.imshow(preds.squeeze(), cmap=\"gray\")\n",
        "        plt.show()\n",
        "\n",
        "        patients = ['patient0001_2CH_ED', 'patient0001_2CH_ES', 'patient0001_4CH_ED', 'patient0001_4CH_ES', 'patient0002_2CH_ED', 'patient0002_2CH_ES', 'patient0002_4CH_ED', 'patient0002_4CH_ES', 'patient0003_2CH_ED', 'patient0003_2CH_ES', 'patient0003_4CH_ED', 'patient0003_4CH_ES', 'patient0004_2CH_ED', 'patient0004_2CH_ES', 'patient0004_4CH_ED', 'patient0004_4CH_ES', 'patient0005_2CH_ED', 'patient0005_2CH_ES', 'patient0005_4CH_ED', 'patient0005_4CH_ES', 'patient0006_2CH_ED', 'patient0006_2CH_ES', 'patient0006_4CH_ED', 'patient0006_4CH_ES', 'patient0007_2CH_ED', 'patient0007_2CH_ES', 'patient0007_4CH_ED', 'patient0007_4CH_ES', 'patient0008_2CH_ED', 'patient0008_2CH_ES', 'patient0008_4CH_ED', 'patient0008_4CH_ES', 'patient0009_2CH_ED', 'patient0009_2CH_ES', 'patient0009_4CH_ED', 'patient0009_4CH_ES', 'patient0010_2CH_ED', 'patient0010_2CH_ES', 'patient0010_4CH_ED', 'patient0010_4CH_ES', 'patient0011_2CH_ED', 'patient0011_2CH_ES', 'patient0011_4CH_ED', 'patient0011_4CH_ES', 'patient0012_2CH_ED', 'patient0012_2CH_ES', 'patient0012_4CH_ED', 'patient0012_4CH_ES', 'patient0013_2CH_ED', 'patient0013_2CH_ES', 'patient0013_4CH_ED', 'patient0013_4CH_ES', 'patient0014_2CH_ED', 'patient0014_2CH_ES', 'patient0014_4CH_ED', 'patient0014_4CH_ES', 'patient0015_2CH_ED', 'patient0015_2CH_ES', 'patient0015_4CH_ED', 'patient0015_4CH_ES', 'patient0016_2CH_ED', 'patient0016_2CH_ES', 'patient0016_4CH_ED', 'patient0016_4CH_ES', 'patient0017_2CH_ED', 'patient0017_2CH_ES', 'patient0017_4CH_ED', 'patient0017_4CH_ES', 'patient0018_2CH_ED', 'patient0018_2CH_ES', 'patient0018_4CH_ED', 'patient0018_4CH_ES', 'patient0019_2CH_ED', 'patient0019_2CH_ES', 'patient0019_4CH_ED', 'patient0019_4CH_ES', 'patient0020_2CH_ED', 'patient0020_2CH_ES', 'patient0020_4CH_ED', 'patient0020_4CH_ES', 'patient0021_2CH_ED', 'patient0021_2CH_ES', 'patient0021_4CH_ED', 'patient0021_4CH_ES', 'patient0022_2CH_ED', 'patient0022_2CH_ES', 'patient0022_4CH_ED', 'patient0022_4CH_ES', 'patient0023_2CH_ED', 'patient0023_2CH_ES', 'patient0023_4CH_ED', 'patient0023_4CH_ES', 'patient0024_2CH_ED', 'patient0024_2CH_ES', 'patient0024_4CH_ED', 'patient0024_4CH_ES', 'patient0025_2CH_ED', 'patient0025_2CH_ES', 'patient0025_4CH_ED', 'patient0025_4CH_ES', 'patient0026_2CH_ED', 'patient0026_2CH_ES', 'patient0026_4CH_ED', 'patient0026_4CH_ES', 'patient0027_2CH_ED', 'patient0027_2CH_ES', 'patient0027_4CH_ED', 'patient0027_4CH_ES', 'patient0028_2CH_ED', 'patient0028_2CH_ES', 'patient0028_4CH_ED', 'patient0028_4CH_ES', 'patient0029_2CH_ED', 'patient0029_2CH_ES', 'patient0029_4CH_ED', 'patient0029_4CH_ES', 'patient0030_2CH_ED', 'patient0030_2CH_ES', 'patient0030_4CH_ED', 'patient0030_4CH_ES', 'patient0031_2CH_ED', 'patient0031_2CH_ES', 'patient0031_4CH_ED', 'patient0031_4CH_ES', 'patient0032_2CH_ED', 'patient0032_2CH_ES', 'patient0032_4CH_ED', 'patient0032_4CH_ES', 'patient0033_2CH_ED', 'patient0033_2CH_ES', 'patient0033_4CH_ED', 'patient0033_4CH_ES', 'patient0034_2CH_ED', 'patient0034_2CH_ES', 'patient0034_4CH_ED', 'patient0034_4CH_ES', 'patient0035_2CH_ED', 'patient0035_2CH_ES', 'patient0035_4CH_ED', 'patient0035_4CH_ES', 'patient0036_2CH_ED', 'patient0036_2CH_ES', 'patient0036_4CH_ED', 'patient0036_4CH_ES', 'patient0037_2CH_ED', 'patient0037_2CH_ES', 'patient0037_4CH_ED', 'patient0037_4CH_ES', 'patient0038_2CH_ED', 'patient0038_2CH_ES', 'patient0038_4CH_ED', 'patient0038_4CH_ES', 'patient0039_2CH_ED', 'patient0039_2CH_ES', 'patient0039_4CH_ED', 'patient0039_4CH_ES', 'patient0040_2CH_ED', 'patient0040_2CH_ES', 'patient0040_4CH_ED', 'patient0040_4CH_ES', 'patient0041_2CH_ED', 'patient0041_2CH_ES', 'patient0041_4CH_ED', 'patient0041_4CH_ES', 'patient0042_2CH_ED', 'patient0042_2CH_ES', 'patient0042_4CH_ED', 'patient0042_4CH_ES', 'patient0043_2CH_ED', 'patient0043_2CH_ES', 'patient0043_4CH_ED', 'patient0043_4CH_ES', 'patient0044_2CH_ED', 'patient0044_2CH_ES', 'patient0044_4CH_ED', 'patient0044_4CH_ES', 'patient0045_2CH_ED', 'patient0045_2CH_ES', 'patient0045_4CH_ED', 'patient0045_4CH_ES', 'patient0046_2CH_ED', 'patient0046_2CH_ES', 'patient0046_4CH_ED', 'patient0046_4CH_ES', 'patient0047_2CH_ED', 'patient0047_2CH_ES', 'patient0047_4CH_ED', 'patient0047_4CH_ES', 'patient0048_2CH_ED', 'patient0048_2CH_ES', 'patient0048_4CH_ED', 'patient0048_4CH_ES', 'patient0049_2CH_ED', 'patient0049_2CH_ES', 'patient0049_4CH_ED', 'patient0049_4CH_ES', 'patient0050_2CH_ED', 'patient0050_2CH_ES', 'patient0050_4CH_ED', 'patient0050_4CH_ES']\n",
        "        name = patients[self.epoch]\n",
        "\n",
        "        torch.save(preds, \"/content/drive/MyDrive/AI4MED/predictions/camus_test/\"+name)\n",
        "              \n",
        "        self.epoch = self.epoch + 1\n",
        "   \n",
        "        return 0\n",
        "\n",
        "    def make_val_pred(self, batch, batch_idx):  \n",
        "        img = batch[0]       \n",
        "        logits = self.model(img.float())  \n",
        "\n",
        "        from torchvision.utils import save_image\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        import cv2\n",
        "        sizes = [(843, 512), (843, 512), (1038, 630), (1038, 630), (973, 591), (973, 591), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1103, 669), (1103, 669), (1103, 669), (1103, 669), (1232, 748), (1232, 748), (1232, 748), (1232, 748), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (1038, 630), (908, 641), (908, 641), (908, 641), (908, 641), (973, 590), (973, 590), (973, 590), (973, 590), (778, 472), (778, 472), (778, 472), (778, 472)]\n",
        "\n",
        "        preds= torch.tensor(cv2.resize(preds.squeeze().cpu().numpy(), dsize=(sizes[self.epoch][1],sizes[self.epoch][0]), interpolation=cv2.INTER_NEAREST)).float().unsqueeze(0)\n",
        "        plt.imshow(preds.squeeze(), cmap=\"gray\")\n",
        "        plt.show()\n",
        "        \n",
        "        patients = ['patient0096', 'patient0096','patient0096','patient0096',\n",
        "                    'patient0105','patient0105','patient0105','patient0105',\n",
        "                    'patient0024', 'patient0024', 'patient0024', 'patient0024', \n",
        "                    'patient0044', 'patient0044', 'patient0044', 'patient0044', \n",
        "                    'patient0031', 'patient0031', 'patient0031', 'patient0031', \n",
        "                    'patient0017', 'patient0017', 'patient0017', 'patient0017', \n",
        "                    'patient0092', 'patient0092', 'patient0092', 'patient0092', \n",
        "                    'patient0004', 'patient0004', 'patient0004', 'patient0004', \n",
        "                    'patient0111', 'patient0111', 'patient0111', 'patient0111', \n",
        "                    'patient0113', 'patient0113', 'patient0113', 'patient0113']\n",
        "\n",
        "        idx = [\"_2CH_ED.mhd\", \"_2CH_ES.mhd\", \"_4CH_ED.mhd\", \"_4CH_ES.mhd\"]\n",
        "        if self.epoch % 4 == 0:\n",
        "          name = patients[self.epoch] + \"_\" + idx[0]\n",
        "\n",
        "        if self.epoch % 4 == 1:\n",
        "          name = patients[self.epoch] + \"_\" + idx[1]\n",
        "\n",
        "        if self.epoch % 4 == 2:\n",
        "          name = patients[self.epoch] + \"_\" + idx[2]\n",
        "\n",
        "        if self.epoch % 4 == 3:\n",
        "          name = patients[self.epoch] + \"_\" + idx[3]\n",
        "\n",
        "        torch.save(preds, \"/content/drive/MyDrive/AI4MED/predictions/new_unet/\"+name)\n",
        "              \n",
        "        self.epoch = self.epoch + 1\n",
        "   \n",
        "        return 0\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr= self.lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lst3-pRmsThw"
      },
      "source": [
        "##**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTNKv7gysRzK"
      },
      "outputs": [],
      "source": [
        "# make use of this implementation for monogenic signals, the paper we cited in \n",
        "# our paper did not have an implementation unfortunately\n",
        "!git clone https://github.com/asp1420/monogenic-cnn-illumination-contrast\n",
        "%cd monogenic-cnn-illumination-contrast/\n",
        "\n",
        "!pip install pyfftw\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.fftpack import fftshift, ifftshift\n",
        "from pyfftw.interfaces.scipy_fftpack import fft2, ifft2\n",
        "import cv2\n",
        "import tools.monogenic_functions as mf\n",
        "from pyfftw.interfaces.scipy_fftpack import fft2, ifft2\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "# train and hyper paramater search\n",
        "for model_name in [\"UTNetV2\"]:\n",
        "  for loss_function in [\"CE+EDGE\"]:\n",
        "    for batch_size in [8]:\n",
        "      for learning_rate in [0.0005, 0.00005]:\n",
        "        for affine in [[0.7, 1.3, 30,0.3],[0.7, 1.3, 30,0.3], [0.9, 1.2, 20, 0.2]]:\n",
        "          for SNR in [True, False]:\n",
        "\n",
        "            datamodule = CAMUS_DATA(batch_size = batch_size, s1=affine[0], s2=affine[1], rotation = affine[2], t=affine[3], SNR=SNR)\n",
        "            model = Segmentation(model=model_name, loss_function=loss_function, weight=[0.5, 1, 1, 1], lr = learning_rate, batch_size=batch_size)\n",
        "\n",
        "            # for saving best model with lowest validation loss, \n",
        "            # a lot of parameters in the name so we dont get confused with if we train\n",
        "            # different variants\n",
        "            checkpoint_callback = ModelCheckpoint(\n",
        "              monitor='D_loss_val',\n",
        "              dirpath=filepath_checkpoint,\n",
        "              filename=str(model.name)+'-{epoch:02d}-{D_loss_val:.6f}-'+str(model.batch_size)+'-'+str(model.loss_function)+str(affine)+str(learning_rate)+str(SNR)+str(loss_function)\n",
        "            )\n",
        "\n",
        "            # plug in your own logger, we used neptune, but we removed our secret api token and project name\n",
        "            #neptune_logger = NeptuneLogger(\n",
        "            #    api_token=\"\",\n",
        "            #    project=\"\", \n",
        "            #    log_model_checkpoints=False\n",
        "            #)\n",
        "\n",
        "            trainer = Trainer(\n",
        "                #logger=neptune_logger,\n",
        "                accelerator=\"auto\",\n",
        "                devices=1 if torch.cuda.is_available() else None,\n",
        "                max_epochs=1,\n",
        "              callbacks=[checkpoint_callback], \n",
        "              log_every_n_steps=1,\n",
        "            )\n",
        "            \n",
        "            # train and validate model\n",
        "            trainer.fit(model = model, datamodule=datamodule)#, ckpt_path=\"/content/drive/MyDrive/AI4MED/model-checkpoints/UTNetV2-epoch=18-val_loss=0.101841-8-CE+EDGE[0.8, 1.2, 20, 0.2]0.0005TrueCE+EDGE.ckpt\")\n",
        "            #neptune_logger.experiment.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3JVdHqbtBNx"
      },
      "source": [
        "##**Load trained model checks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8MWBrs9tM26"
      },
      "outputs": [],
      "source": [
        "datamodule = CAMUS_DATA(batch_size = 1)\n",
        "model = Segmentation(model=\"UTNetV2\", loss_function=\"DICE+CE\", weight=[0.5, 1, 1, 1], lr = 0.0005)\n",
        "\n",
        "trainer.test(model=model, datamodule = datamodule, ckpt_path=\"/content/drive/MyDrive/AI4MED/model-checkpoints/BESTE-UTNetV2-epoch=19-D_loss_val=0.086311-8-CE+EDGE[0.9, 1.2, 20, 0.2]0.0005TrueCE+EDGE.ckpt\")\n",
        "###### Other models used in our paper scores ###\n",
        "#trainer.test(model=model, datamodule = datamodule, ckpt_path=\"/content/drive/MyDrive/AI4MED/model-checkpoints/Unet-epoch=19-D_loss_val=0.098966-8-DICE+CE[0.9, 1.1, 10, 0.1]0.0005FalseDICE+CE.ckpt\")\n",
        "#BESTE-Unet-epoch=18-D_loss_val=0.095380-8-DICE+CE[0.7, 1.3, 30, 0.3]0.0005TrueDICE+CE\n",
        "#trainer.test(model=model, datamodule = datamodule, ckpt_path=\"/content/drive/MyDrive/AI4MED/model-checkpoints/UTNetV2-epoch=18-D_loss_val=0.088782-8-DICE+CE[0.8, 1.2, 20, 0.2]0.0005FalseDICE+CE.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycML95MAYNPq"
      },
      "outputs": [],
      "source": [
        "##### output ugly metrics for previous models\n",
        "total_D1 = []\n",
        "total_D2 = []\n",
        "total_D3 = []\n",
        "total_H1 = []\n",
        "total_H2 = []\n",
        "total_H3 = []\n",
        "for item in model.ES:\n",
        "  D1 = item[0][0]\n",
        "  D2 = item[0][1]\n",
        "  D3 = item[0][2]\n",
        "  H1 = item[1]\n",
        "  H2 = item[2]\n",
        "  H3 = item[3]\n",
        "\n",
        "  total_D1.append(D1)\n",
        "  total_D2.append(D2)\n",
        "  total_D3.append(D3)\n",
        "\n",
        "  total_H1.append(H1)\n",
        "  total_H2.append(H2)\n",
        "  total_H3.append(H3)\n",
        "\n",
        "\n",
        "print(np.mean(total_D1))\n",
        "print(np.mean(total_D2))\n",
        "print(np.mean(total_D3))\n",
        "\n",
        "print(np.mean(total_H1))\n",
        "print(np.mean(total_H2))\n",
        "print(np.mean(total_H3))\n",
        "\n",
        "total_D1 = []\n",
        "total_D2 = []\n",
        "total_D3 = []\n",
        "total_H1 = []\n",
        "total_H2 = []\n",
        "total_H3 = []\n",
        "for item in model.ED:\n",
        "  D1 = item[0][0]\n",
        "  D2 = item[0][1]\n",
        "  D3 = item[0][2]\n",
        "  H1 = item[1]\n",
        "  H2 = item[2]\n",
        "  H3 = item[3]\n",
        "\n",
        "  total_D1.append(D1)\n",
        "  total_D2.append(D2)\n",
        "  total_D3.append(D3)\n",
        "\n",
        "  total_H1.append(H1)\n",
        "  total_H2.append(H2)\n",
        "  total_H3.append(H3)\n",
        "\n",
        "print(np.mean(total_D1))\n",
        "print(np.mean(total_D2))\n",
        "print(np.mean(total_D3))\n",
        "\n",
        "print(np.mean(total_H1))\n",
        "print(np.mean(total_H2))\n",
        "print(np.mean(total_H3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}